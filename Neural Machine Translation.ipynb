{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Zipfile' from 'zipfile' (C:\\Users\\farookjintha\\Anaconda3\\lib\\zipfile.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3749d297410e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mZipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Zipfile' from 'zipfile' (C:\\Users\\farookjintha\\Anaconda3\\lib\\zipfile.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from zipfile import Zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = (\n",
    "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
    "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
    "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
    "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
    "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
    "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
    "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
    "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
    "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
    "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
    "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
    "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
    "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
    "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
    "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
    "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
    "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
    "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
    "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
    "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en, raw_data_fr = list(zip(*raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
    "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
    "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer.fit_on_texts(raw_data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'you': 2, '?': 3, 'the': 4, 'a': 5, 'is': 6, 'he': 7, 'what': 8, 'in': 9, 'do': 10, 'can': 11, 't': 12, 'did': 13, 'giving': 14, 'me': 15, 'i': 16, 'few': 17, 'please': 18, 'this': 19, 'know': 20, 'ridiculous': 21, 'concept': 22, '!': 23, 'your': 24, 'idea': 25, 'not': 26, 'entirely': 27, 'crazy': 28, 'man': 29, 's': 30, 'worth': 31, 'lies': 32, 'very': 33, 'wrong': 34, 'all': 35, 'three': 36, 'of': 37, 'need': 38, 'to': 39, 'that': 40, 'are': 41, 'another': 42, 'chance': 43, 'both': 44, 'tom': 45, 'and': 46, 'mary': 47, 'work': 48, 'as': 49, 'models': 50, 'have': 51, 'minutes': 52, 'could': 53, 'close': 54, 'door': 55, 'plant': 56, 'pumpkins': 57, 'year': 58, 'ever': 59, 'study': 60, 'library': 61, 'don': 62, 'be': 63, 'deceived': 64, 'by': 65, 'appearances': 66, 'excuse': 67, 'speak': 68, 'english': 69, 'people': 70, 'true': 71, 'meaning': 72, 'germany': 73, 'produced': 74, 'many': 75, 'scientists': 76, 'guess': 77, 'whose': 78, 'birthday': 79, 'it': 80, 'today': 81, 'acted': 82, 'like': 83, 'owned': 84, 'place': 85, 'honesty': 86, 'will': 87, 'pay': 88, 'long': 89, 'run': 90, 'how': 91, 'we': 92, 'isn': 93, 'trap': 94, 'believe': 95, 're': 96, 'up': 97}\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
    "                                                        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  5 21 22 23  0  0  0  0  0]\n",
      " [24 25  6 26 27 28  1  0  0  0]\n",
      " [ 5 29 30 31 32  9  8  7  6  1]]\n"
     ]
    }
   ],
   "source": [
    "print(data_en[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
    "fr_tokenizer.fit_on_texts(raw_data_fr_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
    "                                                           padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
    "                                                            padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_fr_in, data_fr_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(20).batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            lstm_size, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, sequence, states):\n",
    "        embed = self.embedding(sequence)\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.lstm_size]),\n",
    "                tf.zeros([batch_size, self.lstm_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            lstm_size, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, state):\n",
    "        embed = self.embedding(sequence)\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
    "        logits = self.dense(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences (1, 8)\n",
      "Encoder outputs (1, 8, 64)\n",
      "Encoder state_h (1, 64)\n",
      "Encoder state_c (1, 64)\n",
      "\n",
      "Destination vocab size 110\n",
      "Destination sequences (1, 7)\n",
      "Decoder outputs (1, 7, 110)\n",
      "Decoder state_h (1, 64)\n",
      "Decoder state_c (1, 64)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "LSTM_SIZE = 64\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "source_input = tf.constant([[1, 3, 5, 7, 2, 0, 0, 0]])\n",
    "initial_state = encoder.init_states(1)\n",
    "encoder_output, en_state_h, en_state_c = encoder(source_input, initial_state)\n",
    "\n",
    "target_input = tf.constant([[1, 4, 6, 9, 2, 0, 0]])\n",
    "decoder_output, de_state_h, de_state_c = decoder(target_input, (en_state_h, en_state_c))\n",
    "\n",
    "print('Source sequences', source_input.shape)\n",
    "print('Encoder outputs', encoder_output.shape)\n",
    "print('Encoder state_h', en_state_h.shape)\n",
    "print('Encoder state_c', en_state_c.shape)\n",
    "\n",
    "print('\\nDestination vocab size', fr_vocab_size)\n",
    "print('Destination sequences', target_input.shape)\n",
    "print('Decoder outputs', decoder_output.shape)\n",
    "print('Decoder state_h', de_state_h.shape)\n",
    "print('Decoder state_c', de_state_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
    "    with tf.GradientTape() as tape:\n",
    "        en_outputs = encoder(source_seq, en_initial_states)\n",
    "        en_states = en_outputs[1:]\n",
    "        de_states = en_states\n",
    "\n",
    "        de_outputs = decoder(target_seq_in, de_states)\n",
    "        logits = de_outputs[0]\n",
    "        loss = loss_func(target_seq_out, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_initial_states = encoder.init_states(1)\n",
    "    en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
    "\n",
    "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "    de_state_h, de_state_c = en_outputs[1:]\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_state_h, de_state_c = decoder(\n",
    "            de_input, (de_state_h, de_state_c))\n",
    "        de_input = tf.argmax(de_output, -1)\n",
    "        out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 3.8258\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "<end>\n",
      "Epoch 2 Loss 3.6829\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous <end>\n",
      "Epoch 3 Loss 3.2035\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous <end>\n",
      "Epoch 4 Loss 2.8604\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "vous <end>\n",
      "Epoch 5 Loss 3.0880\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous <end>\n",
      "Epoch 6 Loss 3.2039\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous <end>\n",
      "Epoch 7 Loss 3.3446\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "<end>\n",
      "Epoch 8 Loss 3.5534\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "<end>\n",
      "Epoch 9 Loss 2.9590\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "vous <end>\n",
      "Epoch 10 Loss 3.2363\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous <end>\n",
      "Epoch 11 Loss 3.1578\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "vous vous vous <end>\n",
      "Epoch 12 Loss 3.0411\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 13 Loss 3.2114\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 14 Loss 3.2079\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 15 Loss 2.8378\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 16 Loss 2.7944\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 17 Loss 2.3653\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 18 Loss 3.0691\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 19 Loss 2.9383\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 20 Loss 2.5842\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 21 Loss 2.8470\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 22 Loss 2.7879\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 23 Loss 2.7158\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 24 Loss 2.9281\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 25 Loss 2.6702\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "vous vous vous il est <end>\n",
      "Epoch 26 Loss 2.6388\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous vous vous vous il <end>\n",
      "Epoch 27 Loss 3.0377\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous vous a il <end>\n",
      "Epoch 28 Loss 2.5264\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous vous est il . <end>\n",
      "Epoch 29 Loss 2.6663\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous vous vous a est <end>\n",
      "Epoch 30 Loss 2.7089\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous a il <end>\n",
      "Epoch 31 Loss 2.9467\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous vous vous a vous <end>\n",
      "Epoch 32 Loss 2.6120\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous vous vous a est . <end>\n",
      "Epoch 33 Loss 2.3663\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous vous a a est . <end>\n",
      "Epoch 34 Loss 2.5768\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous a a est . <end>\n",
      "Epoch 35 Loss 3.0887\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous a a est . <end>\n",
      "Epoch 36 Loss 2.5610\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous vous a a vous ? <end>\n",
      "Epoch 37 Loss 2.7063\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "vous de est est est . <end>\n",
      "Epoch 38 Loss 2.7424\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous vous a a vous ? <end>\n",
      "Epoch 39 Loss 2.6879\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous a a a les . <end>\n",
      "Epoch 40 Loss 2.2911\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous vous a a il . <end>\n",
      "Epoch 41 Loss 2.1493\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de de est est est est . <end>\n",
      "Epoch 42 Loss 2.1336\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous a est est . <end>\n",
      "Epoch 43 Loss 2.6438\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous de a a est les . <end>\n",
      "Epoch 44 Loss 2.2166\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de de de est est est est est . <end>\n",
      "Epoch 45 Loss 2.3194\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous a a il les . <end>\n",
      "Epoch 46 Loss 2.5206\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous a a vous ? <end>\n",
      "Epoch 47 Loss 1.9645\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "je a a a est . <end>\n",
      "Epoch 48 Loss 1.9065\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de a est est les . <end>\n",
      "Epoch 49 Loss 2.3113\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous de a est est les . <end>\n",
      "Epoch 50 Loss 1.7128\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous de de est est les les . <end>\n",
      "Epoch 51 Loss 2.2021\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "je a a a tous . . <end>\n",
      "Epoch 52 Loss 1.7246\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous de de est est les . <end>\n",
      "Epoch 53 Loss 2.0976\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous de a a est est . . <end>\n",
      "Epoch 54 Loss 2.1192\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de de de est est est . <end>\n",
      "Epoch 55 Loss 2.3227\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "je la a a tous est les . <end>\n",
      "Epoch 56 Loss 2.0274\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de de de de c c est est l . <end>\n",
      "Epoch 57 Loss 2.1266\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous de a a tous les . . <end>\n",
      "Epoch 58 Loss 1.7412\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de de est est les . <end>\n",
      "Epoch 59 Loss 2.1682\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous a a est est . <end>\n",
      "Epoch 60 Loss 1.7564\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "de de a tous les les . <end>\n",
      "Epoch 61 Loss 1.7031\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous a a la vous ? <end>\n",
      "Epoch 62 Loss 1.9528\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous a a est les . <end>\n",
      "Epoch 63 Loss 2.0699\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de de qui c c est l . <end>\n",
      "Epoch 64 Loss 1.9141\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous a a tous les . <end>\n",
      "Epoch 65 Loss 1.6431\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous a a tous les . <end>\n",
      "Epoch 66 Loss 1.8020\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l a a est est . <end>\n",
      "Epoch 67 Loss 1.8611\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de a tous les les . <end>\n",
      "Epoch 68 Loss 1.7322\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary tous tous les deux deux . <end>\n",
      "Epoch 69 Loss 1.6869\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de de qui c c anniversaire anniversaire aujourd . <end>\n",
      "Epoch 70 Loss 1.3342\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de de qui c c est l . <end>\n",
      "Epoch 71 Loss 1.6970\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary tous tous les deux deux . <end>\n",
      "Epoch 72 Loss 1.6909\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l a a est est . <end>\n",
      "Epoch 73 Loss 1.6613\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de de tous les les les deux . <end>\n",
      "Epoch 74 Loss 1.2915\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary travaillent tous les deux deux . <end>\n",
      "Epoch 75 Loss 1.3836\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary mary tous tous les deux deux . <end>\n",
      "Epoch 76 Loss 1.7875\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui c c est . <end>\n",
      "Epoch 77 Loss 1.5034\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de de qui c c anniversaire anniversaire aujourd . <end>\n",
      "Epoch 78 Loss 1.3004\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous a la s s ? <end>\n",
      "Epoch 79 Loss 1.3756\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous a la la des ? <end>\n",
      "Epoch 80 Loss 1.6734\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous a la s s ? <end>\n",
      "Epoch 81 Loss 1.4613\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire les les . <end>\n",
      "Epoch 82 Loss 1.7042\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous a la la les . <end>\n",
      "Epoch 83 Loss 1.6350\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary mary tous les deux deux . <end>\n",
      "Epoch 84 Loss 1.2398\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire les les les . <end>\n",
      "Epoch 85 Loss 1.3986\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous de a la tous les . <end>\n",
      "Epoch 86 Loss 1.3809\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l a a la tous . <end>\n",
      "Epoch 87 Loss 1.4253\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de excuser s s les les . <end>\n",
      "Epoch 88 Loss 1.1358\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l a a la tous . <end>\n",
      "Epoch 89 Loss 1.2340\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous qu il s s s s il un piege ? <end>\n",
      "Epoch 90 Loss 1.3137\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous de excuser excuser tous les . <end>\n",
      "Epoch 91 Loss 1.2497\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous de excuser excuser savez vous parler ? <end>\n",
      "Epoch 92 Loss 1.1897\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de faire tous les les . <end>\n",
      "Epoch 93 Loss 1.3563\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il il il il il il endroit . <end>\n",
      "Epoch 94 Loss 1.3637\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "l de de cela tous les les . <end>\n",
      "Epoch 95 Loss 1.2046\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "valeur valeur valeur homme homme dans dans ce est . <end>\n",
      "Epoch 96 Loss 0.9882\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de de travaillent tous les les . <end>\n",
      "Epoch 97 Loss 1.0761\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous vous a la la ? <end>\n",
      "Epoch 98 Loss 1.0466\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "l de de de tous les les . <end>\n",
      "Epoch 99 Loss 1.2829\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous il a la s s piege ? <end>\n",
      "Epoch 100 Loss 1.0339\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous il a a la la bibliotheque ? <end>\n",
      "Epoch 101 Loss 1.2361\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il il est est . <end>\n",
      "Epoch 102 Loss 0.9931\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire tous les les . <end>\n",
      "Epoch 103 Loss 1.1893\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire tous les les . <end>\n",
      "Epoch 104 Loss 0.9176\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire tous les les . <end>\n",
      "Epoch 105 Loss 1.1491\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il il est est . <end>\n",
      "Epoch 106 Loss 0.9089\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary mary tous les deux deux mannequins . <end>\n",
      "Epoch 107 Loss 0.8613\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de de travaillent tous les les . <end>\n",
      "Epoch 108 Loss 1.0303\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous vous de faire tous tous les les . <end>\n",
      "Epoch 109 Loss 1.2188\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui c est . <end>\n",
      "Epoch 110 Loss 0.7762\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent cela cela cela veut . <end>\n",
      "Epoch 111 Loss 1.0544\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous qu il s s s s s il possedait l . <end>\n",
      "Epoch 112 Loss 1.1846\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire hui <end>\n",
      "Epoch 113 Loss 0.7722\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous vous de faire tous tous les les . <end>\n",
      "Epoch 114 Loss 0.9735\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous de excuser excuser tous les trois . <end>\n",
      "Epoch 115 Loss 0.8487\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous de de faire tous tous les les . <end>\n",
      "Epoch 116 Loss 1.0277\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de de faire cela les les trois . <end>\n",
      "Epoch 117 Loss 0.9900\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 118 Loss 0.8971\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 119 Loss 0.9462\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui c l anniversaire . <end>\n",
      "Epoch 120 Loss 0.9154\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de excuser excuser savez vous parler anglais ? <end>\n",
      "Epoch 121 Loss 0.9506\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous de m excuser excuser tous vous parler anglais ? <end>\n",
      "Epoch 122 Loss 0.6394\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de de travaillent tous les deux mannequins . <end>\n",
      "Epoch 123 Loss 0.7425\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 124 Loss 0.9054\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s il s il il possedait endroit . <end>\n",
      "Epoch 125 Loss 1.0350\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee est pas pas completement . <end>\n",
      "Epoch 126 Loss 0.8645\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous laissez a la croire vous abandonniez . <end>\n",
      "Epoch 127 Loss 0.9463\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous de excuser excuser savez vous parler anglais ? <end>\n",
      "Epoch 128 Loss 0.8437\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que veut reellement dire . <end>\n",
      "Epoch 129 Loss 0.7374\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous de m excuser excuser tous vous parler anglais ? <end>\n",
      "Epoch 130 Loss 0.7595\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s il s il il possedait l endroit . <end>\n",
      "Epoch 131 Loss 0.6856\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il il s s s pas d un piege ? <end>\n",
      "Epoch 132 Loss 0.6893\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 133 Loss 0.6720\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous de faire tous les les . <end>\n",
      "Epoch 134 Loss 0.6786\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de de faire cela tous les trois . <end>\n",
      "Epoch 135 Loss 0.5629\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous de excuser excuser tous les trois . <end>\n",
      "Epoch 136 Loss 0.8666\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de excuser excuser savez vous parler anglais ? <end>\n",
      "Epoch 137 Loss 0.6309\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je vous laissez a la par les . <end>\n",
      "Epoch 138 Loss 0.8356\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s il s il il possedait endroit . <end>\n",
      "Epoch 139 Loss 0.7818\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous il s s s s agit un piege piege ? <end>\n",
      "Epoch 140 Loss 0.6754\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous de m excuser tous tous les les . <end>\n",
      "Epoch 141 Loss 0.7725\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous de excuser excuser tous les trois . <end>\n",
      "Epoch 142 Loss 0.6237\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que veut reellement dire . <end>\n",
      "Epoch 143 Loss 0.5490\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 144 Loss 0.6870\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il il est tres mal . <end>\n",
      "Epoch 145 Loss 0.6667\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux mannequins . <end>\n",
      "Epoch 146 Loss 0.6191\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 147 Loss 0.7016\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui c l . <end>\n",
      "Epoch 148 Loss 0.7629\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous de excuser excuser ! savez vous parler ? <end>\n",
      "Epoch 149 Loss 0.5225\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il s s s s pas d un piege ? <end>\n",
      "Epoch 150 Loss 0.5650\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous avez de faire cela tous les . <end>\n",
      "Epoch 151 Loss 0.6101\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous laissez a la par les . <end>\n",
      "Epoch 152 Loss 0.5146\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de excuser excuser ! savez vous parler ? <end>\n",
      "Epoch 153 Loss 0.6667\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous laissez a la croire vous abandonniez . <end>\n",
      "Epoch 154 Loss 0.6531\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de excuser excuser tous les trois . <end>\n",
      "Epoch 155 Loss 0.6149\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s s il s il possedait l endroit . <end>\n",
      "Epoch 156 Loss 0.5561\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous la m excuser ! savez vous ? <end>\n",
      "Epoch 157 Loss 0.5265\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 158 Loss 0.5555\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire aujourd hui <end>\n",
      "Epoch 159 Loss 0.5241\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui <end>\n",
      "Epoch 160 Loss 0.4842\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de faire cela les les . <end>\n",
      "Epoch 161 Loss 0.5427\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous laissez a la par les apparences . <end>\n",
      "Epoch 162 Loss 0.5367\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n pas pas completement . <end>\n",
      "Epoch 163 Loss 0.3981\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de gens travaillent tous les deux . <end>\n",
      "Epoch 164 Loss 0.5467\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de faire cela les les . <end>\n",
      "Epoch 165 Loss 0.5822\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il s s s agit pas d un piege ? <end>\n",
      "Epoch 166 Loss 0.4859\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s il s il possedait l endroit . <end>\n",
      "Epoch 167 Loss 0.4330\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous avez de m excuser tous les trois . <end>\n",
      "Epoch 168 Loss 0.5781\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 169 Loss 0.4293\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de faire cela les les . <end>\n",
      "Epoch 170 Loss 0.4332\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement . <end>\n",
      "Epoch 171 Loss 0.3789\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 172 Loss 0.3870\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 173 Loss 0.5065\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il a la s agit un piege ? <end>\n",
      "Epoch 174 Loss 0.4370\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous avez de m excuser tous les trois . <end>\n",
      "Epoch 175 Loss 0.3933\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que veut reellement dire . <end>\n",
      "Epoch 176 Loss 0.2837\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous avoir minutes je vous prie ? <end>\n",
      "Epoch 177 Loss 0.4404\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il a la s agit un piege ? <end>\n",
      "Epoch 178 Loss 0.3503\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 179 Loss 0.4373\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous laissez a la croire vous abandonniez . <end>\n",
      "Epoch 180 Loss 0.3660\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il a la s s un piege ? <end>\n",
      "Epoch 181 Loss 0.3075\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que veut reellement dire . <end>\n",
      "Epoch 182 Loss 0.4025\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous qu il s s s agit pas d un piege ? <end>\n",
      "Epoch 183 Loss 0.3903\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 184 Loss 0.3671\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 185 Loss 0.3573\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 186 Loss 0.4036\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 187 Loss 0.3111\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que cela veut reellement dire . <end>\n",
      "Epoch 188 Loss 0.3597\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement folle . <end>\n",
      "Epoch 189 Loss 0.3647\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 190 Loss 0.3076\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 191 Loss 0.2893\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 192 Loss 0.3509\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 193 Loss 0.3962\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 194 Loss 0.2985\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s possedait l endroit . <end>\n",
      "Epoch 195 Loss 0.2698\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il a fait tres mal . <end>\n",
      "Epoch 196 Loss 0.3476\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement folle . <end>\n",
      "Epoch 197 Loss 0.3697\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il a fait est . <end>\n",
      "Epoch 198 Loss 0.2736\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l honnetete a la longue . <end>\n",
      "Epoch 199 Loss 0.3324\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 200 Loss 0.2258\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 201 Loss 0.2542\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 202 Loss 0.2431\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 203 Loss 0.3224\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui <end>\n",
      "Epoch 204 Loss 0.2020\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 205 Loss 0.3419\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 206 Loss 0.2288\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous de faire cela tous les apparences . <end>\n",
      "Epoch 207 Loss 0.2552\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 208 Loss 0.2907\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 209 Loss 0.3088\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de gens a tous les deux . <end>\n",
      "Epoch 210 Loss 0.2559\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "je vous de m excuser cela tous les . <end>\n",
      "Epoch 211 Loss 0.3061\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s possedait l endroit . <end>\n",
      "Epoch 212 Loss 0.2935\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 213 Loss 0.2913\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui <end>\n",
      "Epoch 214 Loss 0.2295\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement folle . <end>\n",
      "Epoch 215 Loss 0.3055\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de gens a tous les deux comme . <end>\n",
      "Epoch 216 Loss 0.2081\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il a fait est . <end>\n",
      "Epoch 217 Loss 0.2982\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 218 Loss 0.2339\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 219 Loss 0.3126\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui <end>\n",
      "Epoch 220 Loss 0.2701\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement folle . <end>\n",
      "Epoch 221 Loss 0.2865\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement folle . <end>\n",
      "Epoch 222 Loss 0.2577\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui ! <end>\n",
      "Epoch 223 Loss 0.2000\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui ! <end>\n",
      "Epoch 224 Loss 0.2070\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 225 Loss 0.2682\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 226 Loss 0.1984\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 227 Loss 0.1821\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 228 Loss 0.2514\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 229 Loss 0.2384\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de gens a tous les deux comme mannequins . <end>\n",
      "Epoch 230 Loss 0.1666\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "de qui l anniversaire aujourd hui ! <end>\n",
      "Epoch 231 Loss 0.2808\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
      "Epoch 232 Loss 0.2165\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance ? <end>\n",
      "Epoch 233 Loss 0.2452\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il a fait est tres mal . <end>\n",
      "Epoch 234 Loss 0.1983\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
      "Epoch 235 Loss 0.1691\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 236 Loss 0.2306\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 237 Loss 0.1918\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 238 Loss 0.2077\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il a fait est tres mal . <end>\n",
      "Epoch 239 Loss 0.2290\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 240 Loss 0.2319\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "devine de qui c est l aujourd hui ! <end>\n",
      "Epoch 241 Loss 0.1454\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 242 Loss 0.2022\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "devine de qui c est l aujourd hui ! <end>\n",
      "Epoch 243 Loss 0.1685\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre n est pas completement folle . <end>\n",
      "Epoch 244 Loss 0.2005\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "devine de qui c est l aujourd hui ! <end>\n",
      "Epoch 245 Loss 0.1844\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 246 Loss 0.2492\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "je vous qu il s agit pas d un piege ? <end>\n",
      "Epoch 247 Loss 0.1552\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 248 Loss 0.1349\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 249 Loss 0.2030\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 250 Loss 0.2098\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 250\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    en_initial_states = encoder.init_states(BATCH_SIZE)\n",
    "\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out, en_initial_states)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))\n",
    "    \n",
    "    try:\n",
    "        predict()\n",
    "    except Exception:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
